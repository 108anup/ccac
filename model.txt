So far what we have:

- time is t.
- C = f(queue_length) for a short period of time. f is a monotonically increasing function.
  - if q' > Q then f(q') = f(Q) = link rate
  - in reality: C = f(queue_length, time)  df/dt is small
- Ct - K1 <= Ops(t) <= Ct + K2
- The server can waste at most at rate C

Proved some theorems about what K1 and K2 would be if we compose two servers sequentially with infinite buffers.

Internet model:
- Some notion of link rate:
  - long term if you have enough packets in the queue it is constant, 
        ** can drop if there are not enough packets in the queue.
  - Short term it can change (to allow for bursts/noise/etc.)

What if:
- We have a separate fifo for each flow
- the server can non-deterministically pick a fifo as long as 
    - each fifo gets some share of the bandwidth based on how many packets are in it?
    - the server in total has a min/max rate (to model non-work-conserving)

What if:
- We have one fifo shared between everyone.
- There is too much fate-sharing when it comes to noise.
- That is the only place where CC algorithm is responsible for fairness. If there is a scheduler, that would be the scheduler's responsibility. In that case, the CC's objective is too just selfishly maximize its rate. 
- To have cases where one flow has noise and one flow doesn't, we can have a per-flow non-deterministic delay module. That means packets across different flows can be re-orderd after the fifo (which is very realistic)

Two ways to model slow-down:
- There is fixed function R: #pkt_in_the_queue --> rate
- Wasted tx ops:
  - only waste tx ops when the queue is empty
  - wasting can only happen at rate at most C + (K1 + K2)/RTT:
    - so the server can slowly move in the space between lower and upper bound.
  - changed to wasting can only happen at rate at most C because the worst case seemed to be as bad with the previous choice.

